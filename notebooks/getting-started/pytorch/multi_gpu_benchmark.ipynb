{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4a71d06d",
   "metadata": {},
   "source": [
    "# TUTORIAL : PyTorch computation using multiple GPUs\n",
    "\n",
    "Tutorial adapted from [this PyTorch example](https://github.com/pytorch/examples/tree/master/mnist).\n",
    "\n",
    "## Introduction\n",
    "\n",
    "The aim of this tutorial is to use AI TRAINING product to train a simple model, on the [MNIST dataset](https://en.wikipedia.org/wiki/MNIST_database), with the PyTorch library and to compare performances of running it with one GPU versus multiple GPUs.\n",
    "\n",
    "## Prerequities\n",
    "\n",
    "* a Public cloud project\n",
    "* an AI-TRAINING notebook job launched with the PyTorch preset image ([documentation available here](https://docs.ovh.com/gb/en/ai-training/start-use-notebooks/))\n",
    "* the notebook resources should have at least **2 GPUs**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b6b348e",
   "metadata": {},
   "source": [
    "### Step 1: Import PyTorch library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3211a0da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.optim.lr_scheduler import StepLR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91ccb891",
   "metadata": {},
   "source": [
    "### Step 2: Check that you have GPU(s) available on your notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4350be65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0 (Tesla V100S-PCIE-32GB)\n",
      "cuda:1 (Tesla V100S-PCIE-32GB)\n"
     ]
    }
   ],
   "source": [
    "for device_index in range(torch.cuda.device_count()):\n",
    "    device = 'cuda:{}'.format(device_index)\n",
    "    device_name = torch.cuda.get_device_name(device)\n",
    "    print('{} ({})'.format(device, device_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92fb7fe1",
   "metadata": {},
   "source": [
    "### Step 3: Declare the neural network to train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ff9a89b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, 3, 1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
    "        self.dropout1 = nn.Dropout(0.25)\n",
    "        self.dropout2 = nn.Dropout(0.5)\n",
    "        self.fc1 = nn.Linear(9216, 4096)\n",
    "        self.fc2 = nn.Linear(4096, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = self.dropout1(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc2(x)\n",
    "        output = F.log_softmax(x, dim=1)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a13844c6",
   "metadata": {},
   "source": [
    "### Step 4: Declare train and test functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d6478b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, device, train_loader, test_loader, lr=1.0, gamma=0.7):\n",
    "    print()\n",
    "    print('Train {}'.format(device))\n",
    "    optimizer = optim.Adadelta(model.parameters(), lr=lr)\n",
    "    scheduler = StepLR(optimizer, step_size=1, gamma=gamma)\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        train_one_epoch(model, device, train_loader, optimizer, epoch)\n",
    "        test(model, device, test_loader)\n",
    "        scheduler.step()\n",
    "\n",
    "def train_one_epoch(model, device, train_loader, optimizer, epoch):\n",
    "    losses = []\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        losses.append(loss.item())\n",
    "        if batch_idx % 50 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n",
    "    return losses\n",
    "                \n",
    "\n",
    "def test(model, device, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25a2af71",
   "metadata": {},
   "source": [
    "### Step 5: Load MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c97e7f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform=transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "])\n",
    "dataset1 = datasets.MNIST('/workspace/data', train=True, download=True, transform=transform)\n",
    "dataset2 = datasets.MNIST('/workspace/data', train=False, transform=transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49fabeda",
   "metadata": {},
   "source": [
    "### Step 6: Train model with multiple GPUs\n",
    "\n",
    "There are several ways to use multiple GPUs to train a model, in this notebook we will use [nn.DataParallel](https://pytorch.org/tutorials/beginner/former_torchies/parallelism_tutorial.html).\n",
    "It is not the best way to use several GPUs, [nn.DistributedDataParallel](https://pytorch.org/tutorials/intermediate/ddp_tutorial.html) should give better performances AND can be used to scale on several machines. But DDP does not work well inside Notebooks.\n",
    "\n",
    "We will train our model for only one epoch to make the benchmark run fast but you can increase this value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cebd7928",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train cuda:0\n",
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.301009\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 0.261098\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.170102\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.168584\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.098477\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.132298\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.147196\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.079168\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.153439\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.080951\n",
      "\n",
      "Test set: Average loss: 0.0528, Accuracy: 9820/10000 (98%)\n",
      "\n",
      "\n",
      "Train cuda\n",
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.324143\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 0.251054\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.164459\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.180617\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.133250\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.146410\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.103676\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.103413\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.135682\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.069097\n",
      "\n",
      "Test set: Average loss: 0.0509, Accuracy: 9835/10000 (98%)\n",
      "\n",
      "Mono GPU took 12.42s\n",
      "Multi GPU took 31.25s\n",
      "Multi GPU is 0.40x times faster than multi GPU to train this model\n"
     ]
    }
   ],
   "source": [
    "import timeit\n",
    "\n",
    "# Input batch size for training\n",
    "batch_size = 128\n",
    "# Input batch size for testing\n",
    "test_batch_size = 1000\n",
    "# Number of epochs to train\n",
    "epochs = 1\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset1, batch_size=batch_size)\n",
    "test_loader = torch.utils.data.DataLoader(dataset2, batch_size=test_batch_size)\n",
    "\n",
    "# Mono GPU benchmark\n",
    "device = 'cuda:0'\n",
    "model = Net().to(device)\n",
    "variables = {\n",
    "    'model': model, 'device': device, 'train_loader': train_loader, 'test_loader': test_loader,\n",
    "}\n",
    "mono_gpu_time = timeit.timeit(f'train(model, device, train_loader, test_loader)', globals=variables, number=1, setup=\"from __main__ import train\")\n",
    "\n",
    "# Multi GPU benchmark\n",
    "device = 'cuda'\n",
    "# -- Wrap the model with nn.DataParallel --\n",
    "model = nn.DataParallel(Net()).to(device)\n",
    "variables = {\n",
    "    'model': model, 'device': device, 'train_loader': train_loader, 'test_loader': test_loader,\n",
    "}\n",
    "multi_gpu_time = timeit.timeit(f'train(model, device, train_loader, test_loader)', globals=variables, number=1, setup=\"from __main__ import train\")\n",
    "\n",
    "# Results\n",
    "print('Mono GPU took {:.2f}s'.format(mono_gpu_time))\n",
    "print('Multi GPU took {:.2f}s'.format(multi_gpu_time))\n",
    "print('Multi GPU is {:.2f}x times faster than multi GPU to train this model'.format(mono_gpu_time / multi_gpu_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aef56c7",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "If you check the resource usage of your job you will see that several GPUs are used.  \n",
    "The training is not faster because `nn.DataParallel` does not always improve performances.  \n",
    "There are better way to distribute the training of your model, like using `nn.DistributedDataParallel`, or carefully selecting which part of the model should be shared between multiple GPUs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eade510",
   "metadata": {},
   "source": [
    "### Going further\n",
    "\n",
    "* For more information about running computations with PyTorch we advise you to follow the [official documentation](https://pytorch.org/docs/stable/index.html).\n",
    "* Resource consumption of your notebook is displayed in a dashboard that you can see. Just execute the following cells to get the URL corresponding to your notebook session. The credencials needed to access this dashboard are the same than those used for the current notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04297650",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "if 'NOTEBOOK_ID' in os.environ:\n",
    "    VARID = \"var-notebook=\" + os.environ['NOTEBOOK_ID']\n",
    "    HOST = os.environ['NOTEBOOK_HOST']\n",
    "    SUBDOMAIN = \"notebook\"\n",
    "else:\n",
    "    VARID =  \"var-job=\" + os.environ['JOB_ID']\n",
    "    HOST = os.environ['JOB_HOST']\n",
    "    SUBDOMAIN = \"job\"\n",
    "\n",
    "\n",
    "print(f'Your resource monitoring dashboard URL is :')\n",
    "print(f'http://{HOST.replace(SUBDOMAIN, \"monitoring\")}/d/gpu/job-monitoring?orgId=1&from=now-5m&{VARID}&to=now')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4553b9a6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Conda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
