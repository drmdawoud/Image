{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TUTORIAL : Tensorflow basic computation using single CPU or GPU\n",
    "\n",
    "## Introduction\n",
    "\n",
    "The aim of this tutorial is to use `AI TRAINING` product to do a **very simple tensor computation** with the `Tensorflow` library and to compare performances of running it over CPU versus GPU.\n",
    "\n",
    "## Prerequities\n",
    "\n",
    "* a Public cloud project\n",
    "* an `AI-TRAINING` notebook job launched with the `Tensorflow 2` preset image ([documentation available here](https://docs.ovh.com/gb/en/ai-training/start-use-notebooks/))\n",
    "* the notebook resources should have at least 1 GPU\n",
    "\n",
    "## In practice\n",
    "\n",
    "### Step 1: Import Tensorflow library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2 : Check that you have GPU(s) available on your notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 GPU device(s) have been found on your notebook :\n",
      "* GPU n°0 whose name is \"/device:GPU:0\"\n",
      "* GPU n°1 whose name is \"/device:GPU:1\"\n",
      "\n",
      "1 CPU device(s) have been found on your notebook :\n",
      "* CPU n°0 whose name is \"/device:CPU:0\"\n"
     ]
    }
   ],
   "source": [
    "# Get the list of all logical GPU device on your notebook\n",
    "GPU_DEVICES = tf.config.list_logical_devices('GPU')\n",
    "# Get the list of all logical CPU device on your notebook\n",
    "CPU_DEVICES = tf.config.list_logical_devices('CPU')\n",
    "# Keep only the names of each GPU devices\n",
    "GPU_DEVICES_NAMES = [x.name for x in GPU_DEVICES]\n",
    "# Keep only the names of each CPU devices\n",
    "CPU_DEVICES_NAMES = [x.name for x in CPU_DEVICES]\n",
    "# The number of GPU devices\n",
    "GPU_DEVICES_NB = len(GPU_DEVICES)\n",
    "# The number of CPU devices\n",
    "CPU_DEVICES_NB = len(CPU_DEVICES)\n",
    "\n",
    "if GPU_DEVICES_NB == 0:\n",
    "    raise SystemError('No GPU device found')\n",
    "else:\n",
    "    print(f'{GPU_DEVICES_NB} GPU device(s) have been found on your notebook :')\n",
    "\n",
    "for nb in range(GPU_DEVICES_NB):\n",
    "    gpu_name = GPU_DEVICES_NAMES[nb]\n",
    "    print(f'* GPU n°{nb} whose name is \"{gpu_name}\"')\n",
    "    \n",
    "print('')\n",
    "    \n",
    "if CPU_DEVICES_NB == 0:\n",
    "    raise SystemError('No CPU device found')\n",
    "else:\n",
    "    print(f'{CPU_DEVICES_NB} CPU device(s) have been found on your notebook :')\n",
    "\n",
    "for nb in range(CPU_DEVICES_NB):\n",
    "    cpu_name = CPU_DEVICES_NAMES[nb]\n",
    "    print(f'* CPU n°{nb} whose name is \"{cpu_name}\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3 : Define the operation to benchmark\n",
    "\n",
    "Here we choose to define a simple function that multiply 2 random vectors of the given length. This is the function that we are going to benchmark over available devices (GPU and CPU)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_multiply(vector_length):\n",
    "    vector_1 = tf.random.normal(vector_length)\n",
    "    vector_2 = tf.random.normal(vector_length)\n",
    "    return vector_1 * vector_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4 : Define the function executing the operation on GPU device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gpu_operation(vector_length):\n",
    "    # If you have several GPU you can select the one to use by changing the used index of GPU_DEVICES_NAMES\n",
    "    with tf.device(GPU_DEVICES_NAMES[0]):\n",
    "        random_multiply(vector_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5 : Define the function executing the operation on CPU device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cpu_operation(vector_length):\n",
    "    # If you have several CPU you can select the one to use by changing the used index of GPU_DEVICES_NAMES\n",
    "    with tf.device(CPU_DEVICES_NAMES[0]):\n",
    "        random_multiply(vector_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step6 : Launch the benchmark of each device over several vectors of different lengths\n",
    "\n",
    "Here we are going to iterate over several lengths of vectors and launch a benchmark both on GPU and CPU to observe on which cases GPU is better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Operations on vector of length 1 are 0.704837925731848x faster on GPU than CPU\n",
      "Operations on vector of length 10 are 0.7908874858684348x faster on GPU than CPU\n",
      "Operations on vector of length 100 are 0.789991316682575x faster on GPU than CPU\n",
      "Operations on vector of length 1000 are 1.0697176208473278x faster on GPU than CPU\n",
      "Operations on vector of length 10000 are 2.9363046057797977x faster on GPU than CPU\n",
      "Operations on vector of length 100000 are 5.222210204326995x faster on GPU than CPU\n",
      "Operations on vector of length 1000000 are 24.825055092427608x faster on GPU than CPU\n",
      "Operations on vector of length 10000000 are 260.8940508924052x faster on GPU than CPU\n"
     ]
    }
   ],
   "source": [
    "import timeit\n",
    "\n",
    "# We run each op once to warm up; see: https://stackoverflow.com/a/45067900\n",
    "cpu_operation([1])\n",
    "gpu_operation([1])\n",
    "\n",
    "for i in range(8):\n",
    "    vector_length = pow(10, i)\n",
    "    cpu_time = timeit.timeit(f'cpu_operation([{vector_length}])', number=20, setup=\"from __main__ import cpu_operation\")\n",
    "    gpu_time = timeit.timeit(f'gpu_operation([{vector_length}])', number=20, setup=\"from __main__ import gpu_operation\")\n",
    "    print(f'Operations on vector of length {vector_length} are {cpu_time/gpu_time}x faster on GPU than CPU')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can conclude :\n",
    "* Computation of scalars (i.e vectors of lenght 1) are more efficient on CPU than GPU\n",
    "* Computation of vectors are quite as efficient on CPU or GPU when length is between 10 and 1000\n",
    "* Computation of vectors whose length is higher than 1000 are much more efficient on GPU than CPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Going further\n",
    "\n",
    "* For more information about running computations over GPU with TensorFlow we advise you to follow the [official documentation](https://www.tensorflow.org/guide/gpu)\n",
    "* **Resource consumption** of your notebook is displayed in a dashboard that you can see. **Just execute the following cells to get the URL corresponding to your notebook** session. The credencials needed to access this dashboard are the same than those used for the current notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "if 'NOTEBOOK_ID' in os.environ:\n",
    "    VARID = \"var-notebook=\" + os.environ['NOTEBOOK_ID']\n",
    "    HOST = os.environ['NOTEBOOK_HOST']\n",
    "    SUBDOMAIN = \"notebook\"\n",
    "else:\n",
    "    VARID =  \"var-job=\" + os.environ['JOB_ID']\n",
    "    HOST = os.environ['JOB_HOST']\n",
    "    SUBDOMAIN = \"job\"\n",
    "\n",
    "\n",
    "print(f'Your resource monitoring dashboard URL is :')\n",
    "print(f'http://{HOST.replace(SUBDOMAIN, \"monitoring\")}/d/gpu/job-monitoring?orgId=1&from=now-5m&{VARID}&to=now')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Conda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
